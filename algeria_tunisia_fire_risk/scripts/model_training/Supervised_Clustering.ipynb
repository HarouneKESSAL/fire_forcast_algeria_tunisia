{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71debbed",
   "metadata": {},
   "source": [
    "# Supervised Clustering & Model Training (Reference Exploration Notebook)\n",
    "\n",
    "This notebook demonstrates supervised classification model training on balanced data.\n",
    "\n",
    "**Note**: For final results, use the `from_scratch.py` scripts instead:\n",
    "- `decision_tree_from_scratch.py`\n",
    "- `random_forest_from_scratch.py`\n",
    "- `knn_from_scratch.py`\n",
    "\n",
    "This notebook uses pre-processed balanced dataset (`engineered_features_tomek_enn_balanced.csv`) and is for interactive exploration only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8029d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded balanced dataset: 61524 samples\n",
      "  Features: 22\n",
      "  Class distribution: [47314 14210]\n",
      "  Fire: 14210 (23.1%), No-fire: 47314 (76.9%)\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-processed Data (already balanced)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "# Paths to pre-processed data\n",
    "DATA_PATH = Path('f:/DATA/DATA_CLEANED/processed/engineered_features_tomek_enn_balanced.csv')\n",
    "\n",
    "# Load balanced dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "X = df.drop(columns=['fire']).values\n",
    "y = df['fire'].values\n",
    "\n",
    "print(f\"âœ“ Loaded balanced dataset: {len(df)} samples\")\n",
    "print(f\"  Features: {X.shape[1]}\")\n",
    "print(f\"  Class distribution: {np.bincount(y.astype(int))}\")\n",
    "print(f\"  Fire: {(y==1).sum()} ({100*(y==1).mean():.1f}%), No-fire: {(y==0).sum()} ({100*(y==0).mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a8557",
   "metadata": {},
   "source": [
    "## SECTION 3: Train Models with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b3cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SETTING UP CROSS-VALIDATION\n",
      "================================================================================\n",
      "âœ“ Cross-validation setup complete\n",
      "âœ“ Results dictionary initialized: 2 models loaded so far\n"
     ]
    }
   ],
   "source": [
    "# SECTION 3: Setup Cross-Validation (run once)\n",
    "# Initialize results dictionary and cross-validation setup\n",
    "print(\"=\"*80)\n",
    "print(\"SETTING UP CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, precision_score, recall_score, f1_score\n",
    "\n",
    "# Setup cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize results dictionary (will be filled by each model's cell)\n",
    "if 'sklearn_results' not in globals():\n",
    "    sklearn_results = {}\n",
    "\n",
    "print(\"âœ“ Cross-validation setup complete\")\n",
    "print(f\"âœ“ Results dictionary initialized: {len(sklearn_results)} models loaded so far\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc270a",
   "metadata": {},
   "source": [
    "# Decision Tree with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f16db3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING: DECISION TREE\n",
      "================================================================================\n",
      "\n",
      "DecisionTree:\n",
      "  Fold 1: ROC-AUC=0.8841, F1=0.7618, Threshold=0.5000\n",
      "  Fold 2: ROC-AUC=0.8945, F1=0.7637, Threshold=0.4375\n",
      "  Fold 3: ROC-AUC=0.8805, F1=0.7587, Threshold=0.6154\n",
      "  Fold 4: ROC-AUC=0.8907, F1=0.7656, Threshold=0.5000\n",
      "  Fold 5: ROC-AUC=0.8793, F1=0.7615, Threshold=0.5556\n",
      "  Mean ROC-AUC: 0.8858 Â± 0.0059\n",
      "  Mean F1: 0.7622 Â± 0.0023\n",
      "\n",
      "âœ“ DecisionTree training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree with Cross-Validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING: DECISION TREE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_name = 'DecisionTree'\n",
    "model = DecisionTreeClassifier(max_depth=15, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "print(f\"\\n{model_name}:\")\n",
    "metrics_per_fold = []\n",
    "\n",
    "fold_num = 0\n",
    "for train_idx, val_idx in cv.split(X, y):\n",
    "    fold_num += 1\n",
    "    X_train_fold = X[train_idx]\n",
    "    X_val_fold = X[val_idx]\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    y_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    roc_auc = roc_auc_score(y_val_fold, y_proba)\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_val_fold, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Find best threshold based on F1\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_threshold_idx = np.argmax(f1_scores)\n",
    "    best_threshold = pr_thresholds[best_threshold_idx] if best_threshold_idx < len(pr_thresholds) else 0.5\n",
    "    \n",
    "    y_pred_tuned = (y_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'precision': precision_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'recall': recall_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'f1': f1_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'threshold': best_threshold\n",
    "    }\n",
    "    \n",
    "    metrics_per_fold.append(metrics)\n",
    "    print(f\"  Fold {fold_num}: ROC-AUC={roc_auc:.4f}, F1={metrics['f1']:.4f}, Threshold={best_threshold:.4f}\")\n",
    "\n",
    "# Compute aggregated metrics\n",
    "agg_metrics = {\n",
    "    'roc_auc_mean': np.mean([m['roc_auc'] for m in metrics_per_fold]),\n",
    "    'roc_auc_std': np.std([m['roc_auc'] for m in metrics_per_fold]),\n",
    "    'pr_auc_mean': np.mean([m['pr_auc'] for m in metrics_per_fold]),\n",
    "    'pr_auc_std': np.std([m['pr_auc'] for m in metrics_per_fold]),\n",
    "    'precision_mean': np.mean([m['precision'] for m in metrics_per_fold]),\n",
    "    'precision_std': np.std([m['precision'] for m in metrics_per_fold]),\n",
    "    'recall_mean': np.mean([m['recall'] for m in metrics_per_fold]),\n",
    "    'recall_std': np.std([m['recall'] for m in metrics_per_fold]),\n",
    "    'f1_mean': np.mean([m['f1'] for m in metrics_per_fold]),\n",
    "    'f1_std': np.std([m['f1'] for m in metrics_per_fold]),\n",
    "    'threshold_mean': np.mean([m['threshold'] for m in metrics_per_fold]),\n",
    "    'threshold_std': np.std([m['threshold'] for m in metrics_per_fold])\n",
    "}\n",
    "\n",
    "sklearn_results[model_name] = {\n",
    "    'model_config': {\n",
    "        'model': model_name,\n",
    "        'source': 'sklearn',\n",
    "        'params': model.get_params()\n",
    "    },\n",
    "    'metrics_per_fold': metrics_per_fold,\n",
    "    'aggregated': agg_metrics\n",
    "}\n",
    "\n",
    "print(f\"  Mean ROC-AUC: {agg_metrics['roc_auc_mean']:.4f} Â± {agg_metrics['roc_auc_std']:.4f}\")\n",
    "print(f\"  Mean F1: {agg_metrics['f1_mean']:.4f} Â± {agg_metrics['f1_std']:.4f}\")\n",
    "print(f\"\\nâœ“ {model_name} training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8554c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING: RANDOM FOREST\n",
      "================================================================================\n",
      "\n",
      "RandomForest:\n",
      "  Fold 1: ROC-AUC=0.9025, F1=0.8089, Threshold=0.3603\n",
      "  Fold 2: ROC-AUC=0.9095, F1=0.8144, Threshold=0.3353\n",
      "  Fold 3: ROC-AUC=0.8957, F1=0.8046, Threshold=0.3442\n",
      "  Fold 4: ROC-AUC=0.9118, F1=0.8208, Threshold=0.3540\n",
      "  Fold 5: ROC-AUC=0.8986, F1=0.8124, Threshold=0.4136\n",
      "  Mean ROC-AUC: 0.9036 Â± 0.0062\n",
      "  Mean F1: 0.8122 Â± 0.0054\n",
      "\n",
      "âœ“ RandomForest training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest with Cross-Validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING: RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_name = 'RandomForest'\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=15, min_samples_leaf=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "print(f\"\\n{model_name}:\")\n",
    "metrics_per_fold = []\n",
    "\n",
    "fold_num = 0\n",
    "for train_idx, val_idx in cv.split(X, y):\n",
    "    fold_num += 1\n",
    "    X_train_fold = X[train_idx]\n",
    "    X_val_fold = X[val_idx]\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    y_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    roc_auc = roc_auc_score(y_val_fold, y_proba)\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_val_fold, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Find best threshold based on F1\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_threshold_idx = np.argmax(f1_scores)\n",
    "    best_threshold = pr_thresholds[best_threshold_idx] if best_threshold_idx < len(pr_thresholds) else 0.5\n",
    "    \n",
    "    y_pred_tuned = (y_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'precision': precision_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'recall': recall_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'f1': f1_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'threshold': best_threshold\n",
    "    }\n",
    "    \n",
    "    metrics_per_fold.append(metrics)\n",
    "    print(f\"  Fold {fold_num}: ROC-AUC={roc_auc:.4f}, F1={metrics['f1']:.4f}, Threshold={best_threshold:.4f}\")\n",
    "\n",
    "# Compute aggregated metrics\n",
    "agg_metrics = {\n",
    "    'roc_auc_mean': np.mean([m['roc_auc'] for m in metrics_per_fold]),\n",
    "    'roc_auc_std': np.std([m['roc_auc'] for m in metrics_per_fold]),\n",
    "    'pr_auc_mean': np.mean([m['pr_auc'] for m in metrics_per_fold]),\n",
    "    'pr_auc_std': np.std([m['pr_auc'] for m in metrics_per_fold]),\n",
    "    'precision_mean': np.mean([m['precision'] for m in metrics_per_fold]),\n",
    "    'precision_std': np.std([m['precision'] for m in metrics_per_fold]),\n",
    "    'recall_mean': np.mean([m['recall'] for m in metrics_per_fold]),\n",
    "    'recall_std': np.std([m['recall'] for m in metrics_per_fold]),\n",
    "    'f1_mean': np.mean([m['f1'] for m in metrics_per_fold]),\n",
    "    'f1_std': np.std([m['f1'] for m in metrics_per_fold]),\n",
    "    'threshold_mean': np.mean([m['threshold'] for m in metrics_per_fold]),\n",
    "    'threshold_std': np.std([m['threshold'] for m in metrics_per_fold])\n",
    "}\n",
    "\n",
    "sklearn_results[model_name] = {\n",
    "    'model_config': {\n",
    "        'model': model_name,\n",
    "        'source': 'sklearn',\n",
    "        'params': model.get_params()\n",
    "    },\n",
    "    'metrics_per_fold': metrics_per_fold,\n",
    "    'aggregated': agg_metrics\n",
    "}\n",
    "\n",
    "print(f\"  Mean ROC-AUC: {agg_metrics['roc_auc_mean']:.4f} Â± {agg_metrics['roc_auc_std']:.4f}\")\n",
    "print(f\"  Mean F1: {agg_metrics['f1_mean']:.4f} Â± {agg_metrics['f1_std']:.4f}\")\n",
    "print(f\"\\nâœ“ {model_name} training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0167d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING: KNN\n",
      "================================================================================\n",
      "\n",
      "KNN:\n",
      "  Fold 1: ROC-AUC=0.8934, F1=0.7737, Threshold=0.6364\n",
      "  Fold 2: ROC-AUC=0.9038, F1=0.7936, Threshold=0.6364\n",
      "  Fold 3: ROC-AUC=0.8914, F1=0.7823, Threshold=0.6364\n",
      "  Fold 4: ROC-AUC=0.9053, F1=0.7960, Threshold=0.6364\n",
      "  Fold 5: ROC-AUC=0.8933, F1=0.7849, Threshold=0.6364\n",
      "  Mean ROC-AUC: 0.8974 Â± 0.0059\n",
      "  Mean F1: 0.7861 Â± 0.0080\n",
      "\n",
      "âœ“ KNN training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train KNN with Cross-Validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING: KNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_name = 'KNN'\n",
    "model = KNeighborsClassifier(n_neighbors=11, metric='manhattan', n_jobs=-1)\n",
    "\n",
    "print(f\"\\n{model_name}:\")\n",
    "metrics_per_fold = []\n",
    "\n",
    "fold_num = 0\n",
    "for train_idx, val_idx in cv.split(X, y):\n",
    "    fold_num += 1\n",
    "    X_train_fold = X[train_idx]\n",
    "    X_val_fold = X[val_idx]\n",
    "    y_train_fold = y[train_idx]\n",
    "    y_val_fold = y[val_idx]\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_val_fold)\n",
    "    y_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    roc_auc = roc_auc_score(y_val_fold, y_proba)\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_val_fold, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Find best threshold based on F1\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    best_threshold_idx = np.argmax(f1_scores)\n",
    "    best_threshold = pr_thresholds[best_threshold_idx] if best_threshold_idx < len(pr_thresholds) else 0.5\n",
    "    \n",
    "    y_pred_tuned = (y_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    metrics = {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'precision': precision_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'recall': recall_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'f1': f1_score(y_val_fold, y_pred_tuned, zero_division=0),\n",
    "        'threshold': best_threshold\n",
    "    }\n",
    "    \n",
    "    metrics_per_fold.append(metrics)\n",
    "    print(f\"  Fold {fold_num}: ROC-AUC={roc_auc:.4f}, F1={metrics['f1']:.4f}, Threshold={best_threshold:.4f}\")\n",
    "\n",
    "# Compute aggregated metrics\n",
    "agg_metrics = {\n",
    "    'roc_auc_mean': np.mean([m['roc_auc'] for m in metrics_per_fold]),\n",
    "    'roc_auc_std': np.std([m['roc_auc'] for m in metrics_per_fold]),\n",
    "    'pr_auc_mean': np.mean([m['pr_auc'] for m in metrics_per_fold]),\n",
    "    'pr_auc_std': np.std([m['pr_auc'] for m in metrics_per_fold]),\n",
    "    'precision_mean': np.mean([m['precision'] for m in metrics_per_fold]),\n",
    "    'precision_std': np.std([m['precision'] for m in metrics_per_fold]),\n",
    "    'recall_mean': np.mean([m['recall'] for m in metrics_per_fold]),\n",
    "    'recall_std': np.std([m['recall'] for m in metrics_per_fold]),\n",
    "    'f1_mean': np.mean([m['f1'] for m in metrics_per_fold]),\n",
    "    'f1_std': np.std([m['f1'] for m in metrics_per_fold]),\n",
    "    'threshold_mean': np.mean([m['threshold'] for m in metrics_per_fold]),\n",
    "    'threshold_std': np.std([m['threshold'] for m in metrics_per_fold])\n",
    "}\n",
    "\n",
    "sklearn_results[model_name] = {\n",
    "    'model_config': {\n",
    "        'model': model_name,\n",
    "        'source': 'sklearn',\n",
    "        'params': model.get_params()\n",
    "    },\n",
    "    'metrics_per_fold': metrics_per_fold,\n",
    "    'aggregated': agg_metrics\n",
    "}\n",
    "\n",
    "print(f\"  Mean ROC-AUC: {agg_metrics['roc_auc_mean']:.4f} Â± {agg_metrics['roc_auc_std']:.4f}\")\n",
    "print(f\"  Mean F1: {agg_metrics['f1_mean']:.4f} Â± {agg_metrics['f1_std']:.4f}\")\n",
    "print(f\"\\nâœ“ {model_name} training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c461e6a",
   "metadata": {},
   "source": [
    "## SECTION 3D: Train KNN (Run independently)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49c771",
   "metadata": {},
   "source": [
    "## SECTION 3C: Train Random Forest (Run independently)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bcfdf",
   "metadata": {},
   "source": [
    "## SECTION 3B: Train Decision Tree (Run independently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45fdf2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPORTING SKLEARN RESULTS FOR COMPARISON WITH FROM-SCRATCH\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Exporting sklearn model results...\n",
      "\n",
      "âœ“ DecisionTree exported\n",
      "  ROC-AUC: 0.8858 Â± 0.0059\n",
      "  PR-AUC:  0.8315 Â± 0.0064\n",
      "  F1:      0.7622 Â± 0.0023\n",
      "  â†’ decisiontree_sklearn_summary.json\n",
      "\n",
      "âœ“ RandomForest exported\n",
      "  ROC-AUC: 0.9036 Â± 0.0062\n",
      "  PR-AUC:  0.8699 Â± 0.0065\n",
      "  F1:      0.8122 Â± 0.0054\n",
      "  â†’ randomforest_sklearn_summary.json\n",
      "\n",
      "âœ“ KNN exported\n",
      "  ROC-AUC: 0.8974 Â± 0.0059\n",
      "  PR-AUC:  0.8567 Â± 0.0071\n",
      "  F1:      0.7861 Â± 0.0080\n",
      "  â†’ knn_sklearn_summary.json\n",
      "\n",
      "âœ“ All 3 sklearn results exported to: f:\\DATA\\results\\supervised\n",
      "\n",
      "âœ… Ready for comparison with from-scratch implementations!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORTING SKLEARN RESULTS FOR COMPARISON WITH FROM-SCRATCH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'sklearn_results' in globals() and sklearn_results:\n",
    "    output_dir = Path('f:/DATA/results/supervised')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Exporting sklearn model results...\")\n",
    "    \n",
    "    exported_count = 0\n",
    "    # Export each model's results as JSON (matching from-scratch format)\n",
    "    for model_name, results in sklearn_results.items():\n",
    "        filename = output_dir / f'{model_name.lower()}_sklearn_summary.json'\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        agg = results['aggregated']\n",
    "        print(f\"\\nâœ“ {model_name} exported\")\n",
    "        print(f\"  ROC-AUC: {agg['roc_auc_mean']:.4f} Â± {agg['roc_auc_std']:.4f}\")\n",
    "        print(f\"  PR-AUC:  {agg['pr_auc_mean']:.4f} Â± {agg['pr_auc_std']:.4f}\")\n",
    "        print(f\"  F1:      {agg['f1_mean']:.4f} Â± {agg['f1_std']:.4f}\")\n",
    "        print(f\"  â†’ {filename.name}\")\n",
    "        exported_count += 1\n",
    "    \n",
    "    print(f\"\\nâœ“ All {exported_count} sklearn results exported to: {output_dir}\")\n",
    "    print(\"\\nâœ… Ready for comparison with from-scratch implementations!\")\n",
    "else:\n",
    "    print(\"âš  sklearn_results not found or empty.\")\n",
    "    print(\"   Make sure to run all 3 model training cells first:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec1a08",
   "metadata": {},
   "source": [
    "## SECTION 4: Export Sklearn Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
