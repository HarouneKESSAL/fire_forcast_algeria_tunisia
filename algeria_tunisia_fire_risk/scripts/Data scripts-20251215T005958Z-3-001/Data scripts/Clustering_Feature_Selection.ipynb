{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34fd5e5",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae713402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Paths\n",
    "BASE = Path.cwd()\n",
    "MERGED_PATH = BASE / 'DATA_CLEANED' / 'processed' / 'merged_dataset.parquet'\n",
    "OUTPUT_DIR = BASE / 'DATA_CLEANED' / 'processed'\n",
    "\n",
    "# Load data\n",
    "print(\"Loading merged dataset...\")\n",
    "df = pd.read_parquet(MERGED_PATH)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns[:10])}... (first 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d9d0a",
   "metadata": {},
   "source": [
    "## Section 2: Remove Target Variable\n",
    "\n",
    "‚ö†Ô∏è **CRITICAL**: Drop the fire column - we don't want to guide clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CLUSTERING FEATURE SELECTION (UNSUPERVISED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Drop target variable\n",
    "df_features = df.drop('fire', axis=1).copy()\n",
    "\n",
    "print(f\"\\n1. Removed target variable 'fire'\")\n",
    "print(f\"   Remaining features: {df_features.shape[1]}\")\n",
    "\n",
    "# Also remove coordinates (they're not environmental features for clustering)\n",
    "coords_to_drop = [c for c in df_features.columns if c in ['latitude', 'longitude']]\n",
    "if coords_to_drop:\n",
    "    print(f\"\\n2. Removed spatial coordinates: {coords_to_drop}\")\n",
    "    df_features = df_features.drop(coords_to_drop, axis=1)\n",
    "    print(f\"   Remaining features: {df_features.shape[1]}\")\n",
    "\n",
    "print(f\"\\n‚úì Data prepared for clustering analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b1662",
   "metadata": {},
   "source": [
    "## Section 3: Variance Analysis\n",
    "\n",
    "Remove features with near-zero variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae65848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VARIANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate variance\n",
    "variances = df_features.var()\n",
    "std_devs = df_features.std()\n",
    "\n",
    "print(f\"\\nFeature Variance Statistics:\")\n",
    "print(f\"  Mean variance: {variances.mean():.4f}\")\n",
    "print(f\"  Median variance: {variances.median():.4f}\")\n",
    "print(f\"  Min variance: {variances.min():.6f}\")\n",
    "print(f\"  Max variance: {variances.max():.4f}\")\n",
    "\n",
    "# Find near-zero variance features\n",
    "variance_threshold = variances.mean() * 0.01  # Features with <1% of mean variance\n",
    "low_var_features = variances[variances < variance_threshold].index.tolist()\n",
    "\n",
    "if low_var_features:\n",
    "    print(f\"\\nFeatures with near-zero variance (< {variance_threshold:.6f}):\")\n",
    "    for feat in low_var_features:\n",
    "        print(f\"  - {feat}: variance={variances[feat]:.6f}, std={std_devs[feat]:.6f}\")\n",
    "    \n",
    "    print(f\"\\nRemoving {len(low_var_features)} low-variance features...\")\n",
    "    df_features = df_features.drop(low_var_features, axis=1)\n",
    "else:\n",
    "    print(f\"\\n‚úì No near-zero variance features found\")\n",
    "\n",
    "print(f\"\\nRemaining features: {df_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cac269",
   "metadata": {},
   "source": [
    "## Section 4: Correlation Analysis\n",
    "\n",
    "Remove highly correlated features (keep one from each correlated pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de663791",
   "metadata": {},
   "source": [
    "## Section 4: Correlation Analysis\n",
    "\n",
    "Remove highly correlated features (keep one from each correlated pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8116204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = df_features.corr().abs()\n",
    "\n",
    "# Find pairs with high correlation\n",
    "correlation_threshold = 0.9\n",
    "print(f\"\\nFeatures with correlation > {correlation_threshold}:\")\n",
    "\n",
    "high_corr_pairs = []\n",
    "upper_triangle = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "for i in np.where(upper_triangle):\n",
    "    if corr_matrix.iloc[i[0], i[1]] > correlation_threshold:\n",
    "        feat1 = corr_matrix.index[i[0]]\n",
    "        feat2 = corr_matrix.columns[i[1]]\n",
    "        corr_val = corr_matrix.iloc[i[0], i[1]]\n",
    "        high_corr_pairs.append((feat1, feat2, corr_val))\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr_val:.3f}\")\n",
    "\n",
    "# Strategy: Keep feature with higher mean correlation to other features\n",
    "features_to_drop = set()\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    mean_corr_1 = corr_matrix[feat1].mean()\n",
    "    mean_corr_2 = corr_matrix[feat2].mean()\n",
    "    \n",
    "    if mean_corr_1 > mean_corr_2:\n",
    "        features_to_drop.add(feat2)\n",
    "    else:\n",
    "        features_to_drop.add(feat1)\n",
    "\n",
    "if features_to_drop:\n",
    "    print(f\"\\nRemoving {len(features_to_drop)} redundant features to reduce multicollinearity...\")\n",
    "    df_features = df_features.drop(list(features_to_drop), axis=1)\n",
    "    print(f\"Removed: {list(features_to_drop)}\")\n",
    "else:\n",
    "    print(f\"\\n‚úì No highly correlated features found\")\n",
    "\n",
    "print(f\"\\nRemaining features: {df_features.shape[1]}\")\n",
    "\n",
    "# Visualize remaining correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "new_corr = df_features.corr()\n",
    "sns.heatmap(new_corr, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={'label': 'Correlation'})\n",
    "plt.title(f'Feature Correlation Matrix (After Filtering)\\nn={df_features.shape[1]} features', \n",
    "          fontweight='bold', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633442c",
   "metadata": {},
   "source": [
    "## Section 5: Domain-Based Feature Selection\n",
    "\n",
    "Select features relevant to fire ecology and environmental analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f40eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOMAIN-BASED FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Categorize features by domain\n",
    "feature_categories = {\n",
    "    'Climate': [c for c in df_features.columns if any(x in c.lower() for x in ['temp', 'prec', 'tmax', 'tmin'])],\n",
    "    'Topography': [c for c in df_features.columns if any(x in c.lower() for x in ['elev', 'slope', 'aspect', 'rough'])],\n",
    "    'Soil': [c for c in df_features.columns if any(x in c.upper() for x in ['SAND', 'CLAY', 'SILT', 'CARBON', 'PH', 'CEC', 'BULK'])],\n",
    "    'Landcover': [c for c in df_features.columns if 'lulc' in c.lower() or 'landcover' in c.lower()],\n",
    "    'Engineered': [c for c in df_features.columns if any(x in c.lower() for x in ['index', 'ratio', 'range', 'variability', 'interaction'])]\n",
    "}\n",
    "\n",
    "# Print categories\n",
    "print(f\"\\nFeature Categories:\")\n",
    "for category, features in feature_categories.items():\n",
    "    if features:\n",
    "        print(f\"\\n  {category} ({len(features)} features):\")\n",
    "        for feat in features[:5]:  # Show first 5\n",
    "            print(f\"    - {feat}\")\n",
    "        if len(features) > 5:\n",
    "            print(f\"    ... and {len(features)-5} more\")\n",
    "\n",
    "# Select best features from each category\n",
    "print(f\"\\n\\nSelecting features from each category...\")\n",
    "\n",
    "selected_features = []\n",
    "\n",
    "# Climate: Select most important ones\n",
    "climate_features = feature_categories['Climate']\n",
    "if climate_features:\n",
    "    # Prefer seasonal aggregates over raw monthly\n",
    "    selected_climate = [c for c in climate_features if any(x in c.lower() for x in ['summer', 'winter', 'total', 'avg'])]\n",
    "    selected_features.extend(selected_climate[:4])  # Max 4 climate features\n",
    "    print(f\"  ‚úì Climate: {len(selected_climate[:4])} features\")\n",
    "\n",
    "# Topography: Most important for fire spread\n",
    "topo_features = feature_categories['Topography']\n",
    "if topo_features:\n",
    "    selected_features.extend(topo_features[:4])  # Max 4 topo features (usually: elev, slope, aspect)\n",
    "    print(f\"  ‚úì Topography: {len(topo_features[:4])} features\")\n",
    "\n",
    "# Soil: Select diverse properties\n",
    "soil_features = feature_categories['Soil']\n",
    "if soil_features:\n",
    "    # Prefer variety: texture, chemistry, fertility\n",
    "    soil_texture = [c for c in soil_features if any(x in c for x in ['SAND', 'CLAY', 'SILT'])]\n",
    "    soil_chem = [c for c in soil_features if any(x in c for x in ['PH', 'CARBON', 'WATER'])]\n",
    "    soil_fertility = [c for c in soil_features if any(x in c for x in ['CEC', 'BULK', 'N'])]\n",
    "    \n",
    "    selected_soil = soil_texture[:2] + soil_chem[:2] + soil_fertility[:2]\n",
    "    selected_features.extend(selected_soil)\n",
    "    print(f\"  ‚úì Soil: {len(selected_soil)} features\")\n",
    "\n",
    "# Landcover: All if available\n",
    "lc_features = feature_categories['Landcover']\n",
    "if lc_features:\n",
    "    selected_features.extend(lc_features)\n",
    "    print(f\"  ‚úì Landcover: {len(lc_features)} features\")\n",
    "\n",
    "# Engineered: Add if helpful\n",
    "eng_features = feature_categories['Engineered']\n",
    "if eng_features:\n",
    "    selected_features.extend(eng_features[:3])  # Max 3 engineered\n",
    "    print(f\"  ‚úì Engineered: {len(eng_features[:3])} features\")\n",
    "\n",
    "# Final selection\n",
    "selected_features = list(set(selected_features))  # Remove duplicates\n",
    "if len(selected_features) > 20:\n",
    "    # If too many, select top 20 by variance\n",
    "    selected_features = df_features[selected_features].var().nlargest(20).index.tolist()\n",
    "\n",
    "print(f\"\\n\\nFinal Selected Features ({len(selected_features)} total):\")\n",
    "for i, feat in enumerate(sorted(selected_features), 1):\n",
    "    print(f\"  {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971811f4",
   "metadata": {},
   "source": [
    "## Section 6: Scale and Prepare Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c38b993",
   "metadata": {},
   "source": [
    "## Section 6: Scale and Prepare Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPARATION FOR CLUSTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select final features\n",
    "X_clustering = df_features[selected_features].copy()\n",
    "\n",
    "print(f\"\\nDataset shape: {X_clustering.shape}\")\n",
    "print(f\"Features: {X_clustering.shape[1]}\")\n",
    "print(f\"Samples: {X_clustering.shape[0]}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = X_clustering.isnull().sum().sum()\n",
    "if missing > 0:\n",
    "    print(f\"\\nFound {missing} missing values, imputing with median...\")\n",
    "    X_clustering = X_clustering.fillna(X_clustering.median())\n",
    "else:\n",
    "    print(f\"\\n‚úì No missing values\")\n",
    "\n",
    "# Standardization for clustering (important for distance-based algorithms)\n",
    "print(f\"\\nStandardizing features (zero-mean, unit-variance)...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_clustering)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=selected_features)\n",
    "\n",
    "print(f\"\\nScaled data statistics:\")\n",
    "print(f\"  Mean: {X_scaled.mean(axis=0).mean():.6f} (should be ~0)\")\n",
    "print(f\"  Std: {X_scaled.std(axis=0).mean():.6f} (should be ~1)\")\n",
    "print(f\"\\n‚úì Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fd1fce",
   "metadata": {},
   "source": [
    "## Section 7: Save Clustering Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3895dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING CLUSTERING DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import json\n",
    "\n",
    "# Save standardized data\n",
    "output_path = OUTPUT_DIR / 'dataset_for_clustering.parquet'\n",
    "X_scaled_df.to_parquet(output_path)\n",
    "print(f\"\\n‚úì Saved standardized clustering dataset:\")\n",
    "print(f\"  Path: {output_path}\")\n",
    "print(f\"  Shape: {X_scaled_df.shape}\")\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata = {\n",
    "    'selected_features': selected_features,\n",
    "    'n_features': len(selected_features),\n",
    "    'feature_categories': {\n",
    "        category: [f for f in features if f in selected_features]\n",
    "        for category, features in feature_categories.items()\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'removed_low_variance': low_var_features if 'low_var_features' in locals() else [],\n",
    "        'removed_correlated': list(features_to_drop) if 'features_to_drop' in locals() else [],\n",
    "        'standardization': 'StandardScaler (mean=0, std=1)',\n",
    "        'removed_target': True,\n",
    "        'removed_coordinates': True\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = OUTPUT_DIR / 'clustering_feature_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=2)\n",
    "print(f\"\\n‚úì Saved feature metadata:\")\n",
    "print(f\"  Path: {metadata_path}\")\n",
    "\n",
    "# Save scaler for later use\n",
    "import pickle\n",
    "scaler_path = OUTPUT_DIR / 'clustering_scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"\\n‚úì Saved StandardScaler:\")\n",
    "print(f\"  Path: {scaler_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTERING DATASET PREPARATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  ‚úì Selected {len(selected_features)} features (out of {df_features.shape[1]})\")\n",
    "print(f\"  ‚úì Removed low-variance features\")\n",
    "print(f\"  ‚úì Removed highly correlated features\")\n",
    "print(f\"  ‚úì Removed target variable (unsupervised)\")\n",
    "print(f\"  ‚úì Standardized all features\")\n",
    "print(f\"\\n‚úÖ Ready for: K-Means, DBSCAN, CLARANS clustering\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
